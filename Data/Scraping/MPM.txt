1.1. Accessing Momentum 

Momentum is a web-based system that is accessible via a web browser. To launch Momentum, point your browser address to: 
http://<public-ip-or-domain>:8800/mv-admin 
If you installed Momentum from AWS marketplace, the default port to access Momentum is 8800. 
The above URL will launch the login page.  


1.2. User Registration  

To register for an account: 

Click “Sign Up” link located on the login page. 


Fill out the sign-up form and click “Sign Up” button. 


An activation email is sent your email address.  


In your email, click “Activate Account”. 


After the account is activated, click on the Login link. 


Using the credentials (username and password) that you created during the sign-up process, login to Momentum. 



1.3. Settings and Configuration 

The following settings must be performed before using Momentum. If these settings are not done, some components may not work properly. 
Click the gear icon with your username next to it, located at the right top corner of the screen. Click “Settings” option. This will launch the settings page. 
RSA Key Setting: Momentum components communicate with each other using this secure RSA key. If not set, some components will not work.  

Scroll down to the bottom of the settings page 


Click the button “Generate or Refresh RSA Key” 

After clicking this button, the page will transition to the home page. To perform further settings, click the gear icon and click Settings again. 
Component Settings 
On the settings page, provide the following information for various components to work properly. 
Parameters AWS Default Values MLOPS_URL http://host1.momentum.local:9443 MLOPS_PUBLIC_URL http://<public-ip-or-domain>:9443 MLOPS_TOKEN See the MLOps Token How-tos MLOPS_CLUSTER default_cluster CV_PUBLIC_URL http://<public-ip-or-domain>:5555 GPU_SERVER_HOST host1.momentum.local 

Click “Save Settings” button to save the settings. 


1.4. Securing with SSL 

To secure your Momentum access with SSL: 

Click the gear icon (from top right corner) and select “Settings” option. 


Under the SSL Settings section: 


Click “Enable SSL” checkbox 


Provide SSL Port. Make sure this port is publicly accessible.  


Domain name: make sure the SSL certificate are valid for this domain or sub-domain. 


SSL KeyStore password: The password used in the SSL KeyStore file 


Browse and upload the SSL KeyStore file. 


Click “Enable SSL” button 

Clicking the “Enable SSL” button restarts the Momentum server and it may take a few minutes before Momentum is accessible. 
If everything goes well and SSL certificate is valid for the domain, access Momentum using the https://domain-name:port/mv-admin 
If anything goes wrong and Momentum is not accessible using https, access using the non-SSL URL. You may have to try again if https does not work.  
Some of the common errors with SSL settings could be due to the following reasons: 

The KeyStore password is incorrect 


KeyStore file is invalid 


Port number is invalid or port is blocked by your network firewall. 


Domain name is invalid, incorrect, or the KeyStore file is not valid for this domain. 



Release Notes 

Version 5.0.3, release date: 2023-02-05
Features & Enhancements:

Data upload and exploration: Added features for visual data exploration using graphs and charts.
Emitter: Included Elasticsearch as a data destination/sink.
Data Pipeline is now Data and ML Pipeline that allows to include ML models in the pipeline.
MLOps and deployment related bug fixes.



2.1. Introduction 

Momentum allows ingesting data from a wide variety of data sources and formats. Data can be transformed and prepared for any downstream processes, such as machine learning model training. You can also configure a data pipeline to automate data ingestion and transformation process. 
The following Figure 2.1 shows a high level data engineering component architecture.
Figure 2.1: Momentum Data Engineering Platform Architecture


2.2 Uploading Data 

If you want to ingest data from a file, it should be first uploaded to Momentum server, and then ingested. For example, if you want to ingest data from CSV, Excel, text, JSON, XML or images, the files must be present in the Momentum server. There are two ways to make the files available on Momentum server:  

Via SFTP/SCP upload: large files are recommended to be uploaded via SFTP/SCP. This mechanism also allows to automate the upload process. Your system admin should be able to setup the SFTP/SCP directory on the Momentum server. AWS single node Momentum does not support this feature out of the box. However, to enable this feature, contact our support. 


Via browser drag-and-drop: This is the most convenient method to send the files from your local computer to Momentum. The steps to upload a single file or multiple files are: 


From left hand side navigation panel, click “Data Upload & Exploration”. 


Click “Upload” located on the top menu option. 


Give a directory name where you want to upload the files.  


If the directory exists, the files will be uploaded to that directory.  


If the directory does not exist, it will be created and the files will get uploaded to it. 


To upload to subdirectories, use the back-slash “/” in the directory name. E.g. machine/training directory implies a subdirectory “training” within the parent directory “machine”. 


Either click “Browse Files” or drag one or more files from your local computer’s file system and drop on to the rectangular area indicated so. 


Click “Submit” button to transfer the files.  


It may take a while to upload large files or large number of files. 


Figure 2.2 shows a screenshot example of file upload via drag-and-drop. 


Figure 2.2: Data upload via drag-and-drop
Checking File Upload 
To check if the file/s were uploaded correctly: 

From the “File Upload and Exploration” screen, expand the “Uploaded Files” section and click on the directory name that you created in the previous step. 


If the files were uploaded, you will see a list of all files uploaded successfully.  


Click on the file name to display up to 100 lines of the file.  


Figure 2.3 below illustrates the process of checking the file upload. 


Figure 2.3: Screen to check where files were uploaded successfully 
Important Note
The file upload process needs to be repeated as many times data file needs to be ingested. In other words, every time data needs to be ingested, the file must be uploaded to the directory. 
Also, note that the data from the directory is moved to an archived location and this directory is emptied when an ingester ingests the data, so that the directory is available for a new set of data to be ingested. 
. 
 


2.3 Ingesting Data 

Momentum allows ingesting data from a wide variety of sources and formats. The process of ingesting data from different sources is the same, except that the configuration fields vary for different sources. 
Though the following steps are for ingesting CSV data, the process remains the same for all other data formats and sources. 

Expand “Ingester” from the left navigation panel, and then click “Ingster Home”. 


Click “Ingest Data” located at the top menu option.  


Select “Delimited File” for CSV upload. Select appropriate ingester type based on the data format or system you need to ingest data from. See Figure 2.4 below. 


Figure 2.4: Screenshot showing ingester type selection 

Fill out the form on the next screen.  


Click “Browse Data Files” to launch the screen to select the files to be ingested.  


To select the files, expand the uploaded files, click the directory name, and select the files you want to ingest. If you want to ingest all files from the directory, simply close the popup window without selecting individual files. 


Enter asterisk or * in the file pattern field, and comma “,” in the delimiter field. If the data delimiter is different than comma, enter that. 


Enter max core for parallel ingestion and depending on the data size and number of CPU cores available. For example, 4 cores. 


Submit the form to save the ingester. 


In the next page, select the ingester you just created by clicking the checkbox and click “Run” located at the top menu. 


Click Ingester located at the top to refresh the ingester page to see the running status 


Click “Logs” link corresponding to the ingester to see the running logs – stdout and stderr. 


Important Note

For a file based ingester, the data from the source directory is moved to an archive directory. In other words, after each ingester run, the source directory is emptied. 
For database or external system based ingester, the source data is not moved.



2.4 Transformer 

Momentum provides SQL-based interface for data transformation. Data cleaning, null removal, datatype conversion, column renaming, mathematical transformation, blending, merging, joining with multiple data sources are some of the transformation tasks that can be performed over data created within Momentum. Anything that is supported by ANSI-standard SQL can be performed using Momentum’s transformation engine. A multi-level SQL based transformation is a powerful way of performing complex transformation tasks involving single or multiple data sources. 
In this section we will explore how to perform a single SQL-based as well as multi-step transformations. 
Single Step Transformer 
In the following example, binary formatted date is transformed into human readable date using standard SQL. To do this: 

Form the left hand side navigation panel, expand Transformer” and then click “Transformer Home”. 


Click “New Transformer” located at the top menu option. The transformer window opens. 


The transformer window is divided into two sections: 


The left side section containing a form is used to write transformation SQL, and 


the right-side section shows all data created within Momentum by various components. Clicking on any of the data sources will display 100 rows of data by default.  When the data is displayed, it shows a query block at the top where you can write any SQL statements on top of this datasource to test out any potential transformation query. This view is provided to look at the data while writing the transformation query on the left side section.  See Figure 2.5 below for an example. 


Figure 2.5: View showing 100 records of data to help in writing transformation SQL on the left section 

Notice that the DATE column in the above screenshot is in binary format. We will write SQL to transform that column to a human readable date format. Fill out the form on the left section to configure the transformer as described below (and shown in Figure 2.6 below) 


Figure 2.5: Transformer example showing SQL statement 

Name: give a unique and meaningful name to this transformer 


Transformer Query: Write SQL statement to transform your data. Notice that the table name contains the first two letters of the components that generated data, followed by a dot, followed by the username and another dot, and the name of the component. For example, our datasource table name is “io.sansari.machine_data_ingester” where “io” stands for ingester output”. 


In our example, the SQL statements to convert binary DATE into human readable date is:  

SELECT *, from_unixtime(to_unix_timestamp(`DATE`, ‘yyyy-MM-dd’),’yyyy-MM-dd HH:mm:ss’)  as TRANSFORMED_DATE from  io.sansari.machine_data_ingester 

Select output format, parquet being the default. 


Max core: specify how many CPU core of the cluster your transformer should run on concurrently to perform parallel operations. For example, 4 cores of CPU is good enough for small to mid-size data. For larger dataset, the more that core, the faster the processing will be. 


RAM: specify how much RAM each CPU core should occupy. 4GB per core is a good default for most cases and should be larger for very large dataset. 


Submit to save the transformer configuration. If everything goes well, the page will transition to the Transformer Home page. 


From the Transformer Home page, check the transformer just created, and click the “Run” button located at the top menu bar. 


It might take a few seconds to provision and start the transformation process, and depending on the data size, it make take some time to complete the transformation. 


Click on the “Transformer” menu option from the top menu to refresh the transformer status, shown below in Figure 2.6. 


Figure 2.6: Screen showing the transformation run status. 

Click on the Logs link to monitor the logs and watch for any errors. 


After the transformer is completed successfully, checkbox, and click on “View Data” to take a quick look of the data created by the transformation (see Figure 2.7 below and notice the TRANFORMED_DATE column has human readable dates). See more on data exploration, “Exploring Data”, below.  


Figure 2.7: A sample data generated by the transformer 
Multi Step Transformer 
To demonstrate multi-step transformation process, we will work on an example that creates training and test sets needed for machine learning model training. The steps are as follows. 
Creating Training and Test Sets Using Transformer 
There are many ways to create training and test sets from the dataset we ingested. This section demonstrates how to use SQL compliant transformer to create the two sets for machine learning training and test. 
Assuming we want to create 80% training and 20% test sets from the original 10,000 records. We also want to randomize the data so that the training and test sets are not biased. We will create two transformers: 

The training set transformer will have a randomized 8000 records 


The test set transformer will have 2000 records that are not in the training set. 

Creating Training Set 
Here are the steps to create the training set: 

Expand “Transformer” and click “Transformer Home” to launch the transformer home page. 


Click “New Transformer” from the top menu to open the form to write transformation SQL. Provide a meaningful name to this transformer, e.g. machine_data_training_set 


In the Transformer Query block, write SQL statement as follows. Notice that the SQL statement contains the ingester name or a previously transformed dataset name. 


Save the Transformer. 


Select the transformer and click “Run” located at the top menu bar. 


After the transformer is successfully done, use the data exploration and interactive query tools to check the data. 

SELECT * FROM tr.sansari.machine_data_transformer order by RAND() limit 8000 
Listing 2: Transformer SQL to generate training set 

Figure 2.8: Transformer to create training set 
Notice that the transformer page shows a list of data sources on the right-side panel. You can click on any data item to see the data in tabular form. This view is provided to give an aid to the user while writing the SQL statements for the transformer. 
Creating Test Set 
This section demonstrates a multi-step transformation process, a powerful way of transforming complex data into meaningful forms. Here are the steps to create the test set: 

Click New Transformer at the top menu bar. Give a meaningful name to the transformer, such as machine_data_test_set 


Write a simple “Select” statement to load the data from the original data source, e.g. machine_data_transformer. Logically, the output of this statement will be stored in a temp table called “S1”. 

SELECT * FROM tr.sansari.machine_data_transformer 
Listing 3: SQL statement to load all the data from a transformer 

Click “Add SQL Query” to open another query block. We will write a simple SQL statement to load the data from the training set transformer that we created previously. The output of this SQL is logically stored in another temp table called “S2”. 

SELECT * FROM tr.sansari.machine_data_training_set 
Listing 4: SQL statement to load transformer data that created training set 

The final SQL statement will simply perform a MINUS operation between the two SQL statements above.  

select * from s1 minus select * from s2 
Listing 5: SQL statement to perform MINUS operation 
The following Figure 2.9 shows the three SQL statements as you will see in Momentum Transformer page. 

Figure 2.9: Multi-step transformer to create test set 

Save and run this transformer. Check the result from the Data Upload and Exploration utilities. 

Notes: 

If the column name is a SQL reserved keyword, such as DATE, backquote that column, e.g. `DATE` within the SQL statements. 


The transformer stores data of the last run only. In other words, if you run the transformer multiple times, only the last run output will be stored in transformer.  


Add the transformer in a data pipeline if you want incremental data or full data as a result of the transformation. See “Setting Data Pipeline” section. 



2.5 Custom Processor 


2.6 Emitter: Exporting Data to External System 

Data created by Momentum components are stored on a distributed data lake. Momentum can efficiently access these data for any processing and analysis. However, there are use cases when the processed data from Momentum needs to be exported to a third-party external system. The emitter allows exporting data from Momentum to various external systems. To keep Momentum data warehouse, Impulse, as an independent system, it is also treated as an external system. In other words, data from the lake needs to be exported to Impulse like any other external system. 
This section explains how to configure an emitter that can be attached to a data pipeline (see Setting Up Data Pipeline section) to automate the data ingestion, transformation and export via emitter to external system. 
To configure an emitter: 

Expand Emitter from the main menu option (left hand side menu panel) and click “Emitter Home”. 


Click “Create New Emitter” located at the top menu bar and fill out the config form as explained below. 


Emitter Name: a user defined unique name to identify your emitter. 


Group Name: this field is used in case of IoT to group multiple devices so that their data is collected within the same group. For all other purposes, give any name. 


Memory per Core: How much RAM for parallel process to be used. 1GB should be sufficient for most cases but should be given more for larger dataset. 


Max Core: Number of CPU cores to be used for parallelism. 1 core for small data should be enough. 


Storage Type: Select the appropriate external storage system where you want your data to be exported to. Depending on the type of the storage system, the form fields will different. In this example, the form fields of “Impulse” storage system is explained. 


URL: Give the hostname of Impulse server. For security, give the local hostname as opposed to public DNS or IP address. Make sure the local hostname or IP address is accessible to Momentum server. 


Port: The default port of Impulse is 18888. Use a different port of your admin has installed Impulse to listen on a different port. Make sure Momentum can access this port. 


Warehouse Alias: You must obtain the warehouse alias. See Creating a Warehouse to create a warehouse and alias to be used in this field. The alias name must exist in the data warehouse. 


Table Name: table name where you want to export data to. If the table name does not exist, it be created. 


Username: the authorized username that has at least write permission to the data warehouse 


Password: The authorized user’s password. 


Primary Partition Column: This must be a datatime column. If there is no datetime column, enter __none__ . 


Partition Column Datetime Format: Specify the datetime format. For example: YYYY-MM-DD hh:mm:sss. The datetime should be as specified by jodatime guidelines, https://www.joda.org/joda-time/key_format.html. If your dataset does not have a datetime column and you specified __none__ in the primary partition column field, enter __none__ here as well. 


Partition Granularity: This field is to define the granularity level of your partition either by all, none, second, minute, fifteen_minute, thirty_minute, hour, day, week, month, quarter or year. 


Missing Datatime Placeholder: If there is no datetime column or the datetime value is null or invalid, the datetime will be replaced by the value provided in this field. Leave it default to replace the datetime to current datetime. 


Save Mode: Select the appropriate mode as:  

Overwrite: delete all previous records and recreate new set of records Append: create new rows and append to existing dataset Combine and Overwrite: Combine new data with the old ones and then overwrite. Incremental: Update existing rows if matching primary key is found else create a new row. 

Partition Overwrite Period Start Date: For SaveMode = Combine & Overwrite, specify the start date of the partition that you want to overwrite with the new data. Otherwise, leave it blank. 


Partition Overwrite Period End Date: For SaveMode = Combine & Overwrite, specify the end date of the partition that you want to overwrite with the new data. Otherwise, leave it blank. 


Status: Select active. 


Click Submit to save the emitter config. 

Emitter does not independently run. It always run as a part of a pipeline. 
Creating emitter
Editing An Existing Emitter 

Expand Emitter menu from the left menu options –> click “Emitter Home”  


On the emitter home page, select (checkbox) the emitter you want to edit. 


Click “Edit” located at the top on the emitter home page. 


Edit the form fields as necessary and submit to save the changes. 



2.7 Setting Up a Data Pipeline 

The data pipeline allows automating data ingestion, transformation, ML prediction, and export. You can chain various components in some logical sequence to automate data processing. The data pipeline runs in two modes: On Demand and Scheduled (described below). The data pipeline also allows to attach a custom processor that is build using the Processor interface and packaged as Jar file (more on custom processor below).  
A data pipeline is a sequence of execution of one or more data processing units. For example, a data pipeline may contain one or more ingesters, transformer, custom processing, ML models and emitter. 
To create a data pipeline: 

Create one or more ingesters. See Instructions here. 


Create a transformer that may contain one or more SQL statements within it. Only one transformer per pipeline is allowed. Therefore, all relevant SQL statements must be included in a single transformer. See instructions on how to create a transformer containing multiple SQL statements. 


If your data processing needs any custom processor, create one to be included in the pipeline. See instructions on how to create a processor. 


Create an emitter if the processed data need to be stored outside of Momentum storage (for example, index in Impulse EDW, MongoDB, MySQL, Oracle etc). See instructions on how to create an emitter. 


Create a data pipeline and add all required components to it. See below for more details. 

A few example pipelines: 

one or more ingesters –> one transformer –> one or more processors –> one emitter 
one or more ingesters –> emitter 
one transformer –> emitter 
one transformer –> one or more processors –> emitter 
a single ingester –> emitter 
one or more ingesters –> one transformer –> one or more ML models –> one emitter 

If emitter is omitted, the processed data of the pipeline is stored within the distributed data lake based on HFDS, the main storage system that Momentum utilizes for storing files. 
Creating A Data Pipeline 

Expand “Data Pipeline” menu (under ETL section) from the main menu options –> click “Pipeline Home”. 


Click “Create New Pipeline” from the top menu options 


Fill out the form fields: 


Name: a user defined unique name to identify the pipeline 


Core: Number of cluster cores to execute the pipeline job in distributed and parallel mode. For a big dataset and complex pipeline execution, allocate as much core as you have it available to speed up the execution. 


Memory: RAM per core. 4GB default works for most cases. Tune if required. 


Output Format: If no emitter is attached to this pipeline, the data is stored within the Momentum’s distributed file system (HDFS). Specify the output file format. 


Run Mode: 


On demand: The pipeline needs to be manually executed by clicking the “Run” button. 


Scheduled: Specify a Linux style cron expression to schedule the execution of the pipeline in an automated mode. Here is an online tool to create cron expressions. 


Storage mode: Used only if no emitter is attached to this pipeline. 


Log Input and output Count: If select yes, it will generate the count of processed data for auditing and inventory purpose. This is an expensive process and should be avoided if count is not necessary. 


Submit the form to save it. 


Once the pipeline form is submitted, you will need to add processing units to it. Here are the steps:  


Add one or more ingesters: expand ingester menu –> click on the ingester you want to add –> a rectangular widget is added on the main canvas. 


Add a transformer: expand transformer menu –> click on the transformer you want to add –> a rectangular widget is added on the main canvas. 


To add one or more ML Models, expand the ML models from the left menu panel and click on the models you want to add to the pipeline. 


To add a new processor (not already created): Click “Add Processor” button located at the top of the pipeline canvas. Fill out the form to add to the canvas. 


To add an existing processor: expand process menu –> click on the processor you want to add –> a rectangular widget is added on the main canvas. 


To add a new emitter (not already created): Click “Add Emitter” button located at the top of the pipeline canvas. Fill out the form to add to the canvas. For details on the form field, see the Emitter section of this wiki. 


To add an existing emitter: expand emitter menu –> click on the emitter you want to add to the canvas –> a rectangular widget is added on the main canvas. 


If needed, move the widgets around to organize. Widgets may overlap if the canvas size is small. Drag the overlapped widgets to separate them out. 


Once all widgets are laid out on the canvas, connect them by clicking on the output tip of one widget to the input tail of the other widget. See Figure 2.11 below for an example pipeline with connected units. 


To connect the units, click on the “out” tip and drag the arrow and click on the “in” tip. 


Save the pipeline by clicking the “Save” button. You may need to scroll down to see the “save” button. 

Running Data Pipeline 
To run the data pipeline: 

From the pipeline home page, click on the checkbox corresponding to the pipeline you want to run. 


Click “Run” button located at the top menu bar. 


When the pipeline starts running, it will show the status of execution of each unit that are included in the pipeline. When all the units complete execution, the pipeline status will show as “complete” and result as “success”. 


Figure 2.10: Screen showing pipeline home and menu options 

Figure 2.11: Example pipeline with the connected units (3 ingesters connected with one transformer who output feeds to semantic model that in turn feeds to ANN regression model. The final output is exported to Impulse emitter. 
Important Notes: 
1. A pipeline can contain only one transformer. If you need multiple transformers, write multi-step SQL statements (see Transformer section for details). 
2. Only those models that are deployed to MLOps can be included in the pipeline. If multiple versions of the same model is deployed, it will use the latest version for prediction. 


2.8 Exploring Data 

There are many ways to explore the dataset in Momentum. We will perform a few of them to illustrate the functionality of Momentum. Launch the exploration page by clicking “Data Upload and Exploration” located at the top of the left hand-side menu panel. Here are the steps of data exploration: 
Exploring Data, Types and Distribution 
To understand the data types and column-wise distribution, the steps are: 

Expand the data source, e.g., Ingester Output and select the ingester data you want to explore. 


Click “Explore Data” located at the top menu bar.  


In the next page, the column wise data distribution will display. 


The result shows the data type, total count, number of nulls, min, max, average, and standard deviation of each column. 

A sample data exploration result is shown in Figure 1.12 below. Similarly, data created by other components, such as transformer, machine learning, or NLP, can be explored. 

Figure 2.12 : Data exploration result 
Viewing and Analyzing Data 
Expand the output components, e.g. the “Ingester Output” and click the data component (e.g ingester) you want to explore. This will show 100 records of the data. To show more rows, edit the SQL query shown in the text area and click the ‘blue button’ next to it to run the updated SQL. For example, changing the LIMIT 200 will show 200 rows, ‘LIMIT all’ will show all the data (‘Limit all’ may crash your browser if there is a lot of data). The following Figure 2.13 shows the SQL and data rows. 

Figure 2.13: Data view and corresponding SQL 
Alternatively, you can use Interactive Query to perform ad hoc analysis as described below. 
Ad hoc Analysis Using Interactive Query 
Interactive Query is a powerful data exploration tool that allows you to execute any ANSI-SQL compliant query over data available within Momentum.  
Data within Momentum is organized within the component that generates them. The organization structure is analogous to RDBMS structure in the sense that component name is treated as a database and data generated from various sources as tables of that database. For example, Ingester generated data are organized within “Ingester Output” aliased as “io”. The data tables within the Ingester Output are referenced using fully qualified name as “io.<username>.<tablename>”.  
For example: to explore the machine data to count number of records by Machine_failure, we run the following Interactive Query as shown in Figure 2.14 below. 
SELECT AVG(VIBRATION), NC_MODE FROM io.ai.cnc_historical_data GROUP BY NC_MODE 
Listing 1: Sample SQL statement to count by Machine_failure 

Figure 2.14: Example Interactive Query with sample output 
Visual Analysis 
Visual analysis allows us to plot data to understand the data distribution, outliers, trend, and overall quality of the data. To perform visual analysis, click on “Data Upload & Exploration” and do the following: 

Expand, for example, “Ingester Output”, click on the ingester you want to analyze.  


It will show 100 rows of data. You will notice a graph icon at the top of the query result section (as shown in Figure 2.15 below).  


Clicking on the graph icon will launch a modal window to configure your graph. 


Figure 2.15 Red circle to indicate the graph icon to launch the plot configuration window. 

Figure 2.16: Config example for plotting histogram 

Figure 2.17: An example output of histogram plots 
Downloading Data for Offline Exploration 

Expand, for example, “Ingester Output” or any other component that generated data, select the data you wish to download 


Click “Download Data” located at the top menu bar. 


The data will be downloaded in the format it was originally created, default being the parquet format. 

Note that, depending on the amount of data, it may take a while to generate and download the data from the cluster’s distributed lake to your local computer. 


3.1 Machine Learning 

Momentum is a machine learning automation platform. It has implementation of commonly used machine learning algorithms – supervised, unsupervised, reinforcement learning and recommendation engine. Momentum allows training ML models using drag-and-drop and UI-based approach without writing any code. The following section describes how to train a model, use the trained model for prediction and deploy it to MLOps. Regardless of the algorithm type, the process of training, prediction and deployment is the same for all ML types. 


3.2 Feature Engineering 

There are many algorithms for feature engineering and feature testing. We will explore one of them here. The process described below will be applicable to all feature engineering algorithms available in Momentum. 
Pearson’s Chi-Squared Testing 
To perform hypothesis testing using Pearson’s Chi-squared algorithm, the steps are: 

Under the Machine Learning section, expand “Feature Engineering” and click “Features Home” 


In the next page, select the algorithm, Pearson’s Chi-squared and fill out the form. 


Provide a comma separate list of feature names and the dependent variable. Ensure these fields have numeric data.  


Save by clicking the Submit button. This will create a rectangular widget with the name you specified. 


Expand the data item, such as Ingester, from the left side menu panel, click the ingester to bring it on the main panel. This will create another rectangular widget with the ingester name on it.  


Join the ingester widget to the Chi-squared config widget. First click the “Out” of ingester box, and then click the “In” of the Chi-squared box. Click to save. Refer to Figure 3.9 to see an example. 


Figure 3.9: Pearson’s Chi-squared test setup 

From the Feature Engineering home page, select the configuration you just created, and click “Run” button. 


Click “Feature Engineering” at the top menu bar to refresh the page to see the running status. 


Click Logs to see the log messages.  


After the execution is successful, you can explore the output by clicking the “View Data” or from the Data Upload and Exploration section.  

Figure 3.10 shows the output of Chi-squared test of the machine failure data we used in this example. Scroll to the right to see the full result. 

Figure 3.10: Chi-squared test of hypothesis 


3.3 Model Training 

Traditionally, we split data into training and validation sets in 80:20 ratio. Before we start the model training, make sure you have ingested data into Momentum and created training and validation sets. If you have not done so, here are links that will helps you to prepare the data. 
How to ingest data Momentum 
How to create training and test sets using Transformer 
How to create a data processing pipeline 
To explain the process of training a model, we will use deep learning or artificial neural network based multi-layer perceptron classifier that predicts if a machine will fail given its operating condition. We have ingested the AI4I 2020 Predictive Maintenance Dataset (https://archive.ics.uci.edu/ml/datasets/AI4I+2020+Predictive+Maintenance+Dataset), and created 80% training and 20% test sets.  
To train a model, e.g., multi-layer perceptron classifier, here are the steps: 

Expand “ML Model” from under the “Machine Learning” section of the left side menu panel, and click “ML Home” to launch the ML home page. 


Click “Create New Model”.  


From the “Supervised Learning: Classification” drop down, select “Deep Learning/ Artificial Neural Network/Multilayer Perceptron Classifier.  


Fill out the form as described below, and shown in Figure 3.1a and 3.1b for example: 


Model Name: give a meaningful name to identify this model, for example Machine_Failure_Prediction_Model 


Give a version number, just in case you need to build multiple versions of this model. 


In the Feature Field text area, supply a comma separated list of all features you want the model to learn from. In this example, we are using the following features: 

Air_temperature,Process_temperature,Rotational_speed,Torque,Tool_wear_in_min 
Listing 3.1: Feature list 

Categorical/Non-numerical fields: comma separated list of all features that are not numeric. We will not have such field, so we will leave it empty. 


OneHot Encodable Field: Categorical and non-numeric fields should be encoded. Leave it empty in this case. 


Target Field that needs to be predicted. Machine_failure is our target field. 


Number of Classes: The machine failure data has only two classes – 0 means no failure and 1 means failure. We will fill 2 in this field. 


Scale Features: It is generally a good idea to scale the features, we will select “yes”. 


Feature excluded from scaling: We will leave this field empty. 


Number of hidden layers: This is to configure the neural network. We will start with 3 hidden layers. 


Number of nodes in each hidden layer: We will use varying number of nodes in each layer. For example, 19,11,9 to indicate we want to use 19 nodes in the first hidden layer, 11 in the second and 9 in the last hidden layer. If you want to use the same number of neurons in each hidden layer, use a single number. For example, if we enter 16 in this field, all hidden layers will have 16 neurons. 


Max Number of Iteration: We are starting with 1000. If the algorithm converges before it reaches the max 1000 iterations, the training will automatically stop to avoid unnecessary computation and time.  


Training: Test ratio to split the training data internally into this ratio. We are using 0.8:0.2 for 80% and 20% split. 


Max Core: This is to parallelize the training by using multiple CPU cores of the cluster. We are going to use 16 cores as our dataset is not large. For large dataset, using more or all available CPU cores of the cluster will speed up the training process. 


Memory per Core: For most training 4GB per core should be sufficient but may be increased for a large and complex model. 


The number of partitions is not used at this time and is reserved for the future. 


Figure 3.1a: Showing a part of the neural network configuration form. 

Figure 3.1b: Showing the remaining part of the neural networks form 

After submitting the form, a rectangular config widget is created on the main body of the page. 


Expand Transformer from the left menu panel and click on the training set transformer to bring it to the main page. 


Click on the “Out” on the training set rectangle and click on the “In” of the model config rectangle. This will join the training dataset to the model config (shown in Figure 12 below) 


Saving the configuration will take the screen to the ML home page. 


Click “Run” located at the top menu to start the model training. 


Figure 3.2: Screen showing model configuration 


3.4 Monitoring ML Training 

Machine learning model training is a compute-intensive and time-consuming process. There are a few ways to monitor the training progress. 

Click on ML Model located at the top menu bar or refresh your browser to see the current running status. While the model is running, it will show a spinner indicating the model is learning. 


Click “Logs” to see the runtime log – both stdout and stderr logs are displayed in a separate page. 


After the model is trained (and in some cases, while the model is learning), click the graph icon to show the “Model Score” curve. For the MLPC model, the curve shows a plot of model loss vs iterations. Figure 3.3 shows an example loss graph. 


Figure 3.3: Loss vs iteration graph. 

Click “View Model” to see the model metrics after the training is completed (Figure 3.4 for example) 


Figure 3.4: Model metrics 


3.5 ML Prediction 

Followings are the steps to predict the outcomes from a trained model: 

Click “ML Prediction” on the left menu panel and click “Create New Prediction” from the top menu bar. 


Fill out the form. Make sure to provide the same list of features that were used in the model training. 


Select “No” for recommendation and leave all other fields to default for most cases. Select “yes” if this prediction is for recommendation engine. 


Save the form. 


Expand “Transformer” from the left menu and click on the test data set related transformer. 


Expand “ML Model” and click on the ANN model just trained. 


Connect the two rectangles as shown in Figure 3.5 below. 


Figure 3.5: Model prediction screen 

Submit to save the prediction configuration. 


Select the prediction config and click “Run” located at the top menu bar to execute the prediction job. 


After the prediction is completed, use the data exploration to assess the accuracy of the model. For example, the following interactive query in Figure 3.6 shows an ad hoc analysis done on the test prediction. This SQL statement yields the confusion matrix. 


Figure 3.6: A sample interactive query to analyze the prediction test result 


3.6 Model Deployment 

Momentum provides a 1-click deployment of ML models to MLOps. Before deploying a model to MLOps, make sure that Momentum is properly configured to locate the MLOps deployment.  
Check Settings and Configuration and ensure the following properties are configured correctly: 
Parameters  Example Values  Explanation MLOPS_URL  https://host1.momentum.local:9443  Use internal hostname or IP and not public domain/IP. Contact your admin to obtain the hostname where MLOps is hosted. Momentum internally communicates with MLOps using this host and port and therefore for security reason, use an internal host:port. MLOPS_PUBLIC_URL  http://<public-ip-or-domain>:9443  This is public hostname and port to access the MLOps UI and securely login to access its services. MLOPS_TOKEN  See the MLOps Token How-tos  This is a security token that is used to communicate with MLOps for security.  MLOPS_CLUSTER  default_cluster  MLOps can seamlessly deploy models in various environment such as QA, sandbox, staging, production that may be either local server, Azure, AWS or any other cloud service providers. The deafult_cluster means the model will be hosted within the same machine where MLOps is deployed. 
To deploy a model from Momentum to MLOps: 
From the ML Model home page, select the model you wish to deploy to MLOps, and click “Deploy” button located at the top menu bar.  
The status message will indicate if the model is successfully deployed. Figure 3.7 shows the deployment screen. 

Figure 3.7: Screen showing model deployment. Notice the red circle showing “Deploy” button. 
Login to MLOps to check in the model registry if the model just deployed is listed there. Figure 3.8 shows a screenshot showing our model in the registry. 

Figure 3.8: Model registry of MLOps showing the deployed models and versions. 
Checkout the following for MLOps details: 
Creating MLOps Security Token 
Checking Model Metadata and Prediction API 
Checking Model Quality Report 
Checking Data Drift Report 
Checking Model Operating Report 


4.1 Image Classification 


4.2 Object Detection using SSD 


4.3 Object Detection using YOLO 


Introduction 

MLOps stands for Machine Learning Operations. MLOps streamlines the process of deploying ML models to production, maintaining and monitoring them. MLOps also provides a continuous integration and continuous deployment (CI/CD) for machine learning modeling, testing, and deployment.  
MLOps allows deploying models trained using Momentum as well as models built using third party tools, systems, libraries and programming languages. Third party models must comply with one of the following standards: 

PMML or Predictive Model Markup Language 


ONNX or Open Neural Network Exchange 


TensorFlow 

ML models can be deployed to MLOps in any of the following ways: 

Momentum ML UI 
MLOps UI 
Restful API 



Getting Secure Access to MLOps 

To gain access to MLOps, you must have a valid credentials. Depending upon your organizational policy, sign up functionality may be disabled. In that case, your admin should create your account and assign appropriate privileges. See User and Role Management for details on how to create user and assign roles. 
If sign up is enabled and you want to create a new organization within MLOps, the following steps will guide you to do that: 

On the login page, click the link “Signup” 


Fill out the sign-up form and click “Register” button. 


If everything goes well, your account will be created. 


By default, you will be the admin of the organization. 



Generating Security Token 

The security token is needed for Momentum integration with MLOPs or to use its Restful APIs. You can either see and copy the existing token or reset it (if the token is compromised). Here are the steps: 
4.2.1 Viewing Security Token 

Login to MLOps web interface using your secured credentials. 


On the left hand-side menu panel, click “APIs” menu option. 


In the next page type your account password 


Click the button “Reveal Key” (do not press enter key. Else it will show an error page) 


By default, the API key is hidden. If the password is correct, clicking the Reveal Key button displays the API key that you can copy and paste wherever needed. 

4.2.1 Resetting Security Token 

Login to MLOps web interface using your secured credentials. 


On the left hand-side menu panel, click “APIs” menu option. 


In the next page type your account password. 


Click the button “Reset Key” (do not press enter key. Else it will show an error page) 


By default, the API key is hidden. If the password is correct, clicking the Reset Key button displays the API key that you can copy and paste wherever needed. 



Integrating MLOps with Momentum 

Momentum provides a 1-click deployment of ML models to MLOps. Before deploying a model to MLOps, make sure that Momentum is properly configured to locate the MLOps deployment.   
Check Settings and Configuration and ensure the following properties are configured correctly:  
Parameters   Example Values   Explanation  MLOPS_URL   https://host1.momentum.local:9443   Use internal hostname or IP and not public domain/IP. Contact your admin to obtain the hostname where MLOps is hosted. Momentum internally communicates with MLOps using this host and port and therefore for security reason, use an internal host:port.  MLOPS_PUBLIC_URL   http://<public-ip-or-domain>:9443   This is public hostname and port to access the MLOps UI and securely login to access its services.  MLOPS_TOKEN   See the MLOps Token How-tos   This is a security token that is used to communicate with MLOps for security.   MLOPS_CLUSTER   default_cluster   MLOps can seamlessly deploy models in various environment such as QA, sandbox, staging, production that may be either local server, Azure, AWS or any other cloud service providers. The deafult_cluster means the model will be hosted within the same machine where MLOps is deployed.  


Deploying ML Models via Momentum UI 

See Model Deployment from Momentum to MLOps. 


Deploying ML Models via MLOps UI 

Login to MLOps web interface and follow the steps to deploy a model to MLOps: 

From the left menu panel, click “Deploy” option 


Fill out the form (as shown in Figure 4.1 below). 


Model Name: Give a meaningful name of the model 


Select Model Category: Select from the dropdown the type of model you are deploying. 


Purpose (optional): Give a short description and reason why you are deploying this model  


Business Description (optional): Give a business description about the usage of the model 


Author: by default, the logged in user’s id is pre-filled. However, if the author of the model is someone else in the organization, provide that user’s id. 


Select Cluster Id: Select the cluster where the model needs to be deployed. Depending on your organization’s deployment, the option may vary. Selecting “default_cluster” means, the model will be deployed within the same machine where the MLOps is deployed. 


Choose File: Browse to the location in your machine where the model file is located and select the model to upload. The supported file formats are: 


.xml for PMML compliant models 


.onnx for ONNX compliant models 


.zip for TensorFlow models (make sure all the model artifacts are included and packages as a zip file). 


Click “Deploy” button. 


If everything goes well, you should see the metadata of the model with the “success” confirmation. 


Figure 4.1: Screen showing the model deployment page 


Deploying ML Models via Restful API 

ML models can be deployed programmatically using the Restful API. The Restful API details are also available on the “Deploy” page of MLOps. To see the API details for model deployment, click the “Deploy” menu option and scroll down to the bottom of the page to see the CuRL command and python code. 
Figure 4.2 shows as a screenshot. 

Figure 4.2: Screenshot showing the Curl and Python code for Restful API for model deployment 
Using the command line or CURL:  
curl -X PUT -H ‘Content-Type:multipart/form-data’ -H ‘Authorization: Token 1234567890987654321’ -F ‘model_file=@myfile.xml’ http://one.accure.ai:443/mlops/v1/model_upload/model_name/cluster_id/model_category 
Using Python code: 
url = http://one.accure.ai:443/mlops/v1/model_upload/model_name/cluster_id/model_category  
headers = {“Authorization”: “Token 1234567890987654321”} 
files = {“model_file”: open(r”myfile.xml”, “rb”)} 
r = requests.put(url=url, headers=headers, files=files) 
print(r.json()) 
Replace the token with the real security token.  
Replace the “myfile.xml” by the model file path. 
Replace the URL with your MLOps deployed domain and port. 
model_name: replace with the model name you are deploying 
cluster_id: Use appropriate cluster_id (default_cluster by default) 
model_category: Depending on the category of the model you are deploying. The supported categories are: 
classificateion 
clustering 
image_classification 
object_detection 
nlp 
regression  


Model Registry 

MLOps maintains a list of all deployed models with their corresponding versions in the form a registry, called Model Registry. 
After signing in to MLOps, it lands to the model registry page by default. To navigate back to the registry, click on “Model Registry” menu option. Figure 4.3 shows the model registry page. 

Figure 4.3: Model Registry page showing models and their versions. 
The model registry also shows the “cluster_id” where the model is deployed, the model algorithm name, prediction function, publication date and the author of the model. 


Model Exploration 

To explore the model and view its metadata: 

From the model registry page, click the Model Artifact icon, , corresponding to the model you wish to explore. 


The next page displays the model metadata. 


The model metadata page also shows, depending on the model type, the following: 


Model Upload API to upload a new version of the model programmatically 


Model Artifact API to retrieve the model metadata programmatically 


Model Prediction API to predict from this model by passing the input data to it. 

Figure 4.4 shows the model metadata exploration page. Notice the top menu options on this page. 

Figure 4.4: Model metadata or artifact page 


Feature Store 

Feature Store is a central repository to store the feature data used in training the ML models. After the model is deployed to MLOps, the training set and test sets must be uploaded for auditing, model recreation, audit compliance, and other regulatory and business requirements. 
In addition, the training and test sets must be uploaded to the Feature Store for Data Drift Detection and Model Performance Report. 
To upload the training and test sets to Feature Store: 

If you are on the Model registry page, click the icon corresponding to a model for which the data needs to be uploaded to Feature Store. 


If you are already on the model Artifact or Metadata page, you will see a Feature Store link at the top menu bar. Clicking the Feature Store icon or menu item will bring the Feature Store page. Figure 4.5 shows the Feature Store page. 


Figure 4.5: Feature Store page 

On the Feature Store page: 


Dataset: select either training or test set to upload the relevant dataset 


File Type: Select the file format (json, parquet, or csv) 


Browse to the data file to your local computer and upload by clicking the Submit button. 


If you deployed model directly from Momentum to MLOps, make sure to download the training and test sets from Momentum and upload to Feature Store. 


To download the training or test sets, login to Momentum 


Go to Data Upload and Exploration section 


Select the training or test sets you used in the model 


Click the Download menu item located at the top menu bar. 


The downloaded file will be a zipped file. Unzip it. You may have multiple files within the zipped file. Make sure to upload all of the training or test files one by one. 


If you deployed a third-party model (not from Momentum), make sure to acquire the training and test sets in one of the supported file formats, and the upload to the Feature Store. 


You can also upload the training and test sets to MLOps via Restful API. The API specification, with a sample Python code, is provided on the Feature Store page. Here is a sample CURL command to upload data to Feature Store via Rest API: 

curl -X PUT -H ‘Content-Type:multipart/form-data’ -H ‘Authorization: Token 1234567890987654321‘ -F ‘data_file=@myfile.csv‘ http://one.accure.ai:443/mlops/v1/upload_dataset/LSTM_Test_Model/2/default_cluster/dataset_category/file_type 
Import Note: For Regression models, the training and test sets must have the target field labeled as “target” even if the model was trained with a different column name of the target field. This is likely to change in the future but for now, rename your label field before uploading to Feature Store. You do not need to change anything in your model, Just the training and test datasets. 


Sharing Model 

Model can be shared with other users within the same organization. Only the admin and model owners can share their models with other users. See the “User and Role Management” section for details on the roles and privileges. 
To share a model: 

If you are on the Model Registry page, click on the sharing icon corresponding to the model you want to share. 


This will launch a modal window. Search the user you want to share this model with. 


Click on the icon corresponding to the user you want to share this model with. 


Figure 4.9: Model sharing 
Unshare or Revoke Access to Model 
As shown in Figure 4.9, if a model is already shared with a user, a delete icon is displayed with the user name. Clicking the delete icon will unshare the model with that user. 
Also see “Model Access Management” user the section “User and Role Management”. 


Model Operation Monitoring 

To monitor the model operations: 

If you are on the Model registry page, click the icon to open the Model Ops page. 


If you are on the model metadata or artifact page, click on the “Model Ops” menu item from the top menu bar. 


The Model Ops shows: 


Total Requests: this indicates the number of API calls 


Total Predictions: Number of predictions done by the model. Each API call may send multiple input sets. Predictions are done for each input and hence this shows the number of predictions made by the model for all the inputs received. 


Median Response Time (ms) Per Request: As the name suggests, this is an aggregate median time of all the requests processed by the API. 


Data Error Rate: Any error detected due to data issues, such as incorrect column types, missing or null, the API will record that as errors. The error rate is calculated by the diving the data related error count by the total number of API requests received. 


System Error Rate: This error rate indicates the error due to any system related issues, such as the serving node being down, network issues, or other system related issue not related to the data. 


Consumers: This indicates the number of API consumers. 


Data Error Per Day: This shows a daily trend of error rates. 


The Model Ops report is specific to a particular model. 


The aggregation is done over a week worth of data and older data are removed to save on disk space. This behavior may be configured from the backend. 

Figure 5.6 shows a screenshot of the Model Ops page. 

Figure 4.6: Screenshot showing the Model Ops page 


Model Performance Report  

To monitor the model performance: 

Click “Model Stats” from the top menu bar from the model metadata page. 


Click “Model Performance Report” from the secondary top menu bar. 


Depending on the model type, the performance report shows metrics suitable for that model type. Figure 4.7 shows an example screen of performance of an LSTM based model. 


Figure 4.7: Model Performance report 


Data Drift Report  

The data drift detection is a way to evaluate whether the new incoming data is drifted from the data that was used in the model training. MLOps monitors for drift of every feature – both numerical and categorical features – and report them as graphs.  
Figure 4.8a shows a data drift detection report page. Clicking on each column will show detailed drift (as trend graph, Figure 4.8b). 

Figure 4.8a: Data Drift Detection report page. 

Figure 4.8b: Field specific drift trend. The points within the green band are compliant data while the data outsides the green area are drifted data. 


User and Role Management 

You must have an administrative role for manage users and roles within your organization. Figure 4.10 shows the supported user groups and their corresponding privileges. 

Figure 4.10 User groups and privileges 


Add User 


Expand Users and Roles from the left menu panel. 


Click Manage Users. This opens the user management page. 


Click “Add User” located at the top menu bar. 


Fill out the form and select the appropriate role. 


Save to add this user. 



Edit or Delete User 


Expand Users and Roles from the left menu panel. 


Click Manage Users. This opens the user management page. 


Click the pencil icon corresponding to the user you want to edit or delete. 


Edit the user information and click the “Update” button. 


To delete a user, select the “User Status” as “inactive” and click “Update” button. 


To update the user’s role, select the appropriate role from the “User Group” dropdown, and click “Update” button. 



Model Access Management 


Expand Users and Roles from the left menu panel. 


Expand Model Access, and click either Model View or User View 


Model View shows a list of models and users who have access to these models (Figure 4.11a) 


User View shows a list of users and all the models they have access to (Figure 4.11b) 


Figure 4.11a: Model View of the model access 

Figure 4.11b: User View of the model access 

As shown in Figure 11.a, click on the icon corresponding to the model you want to manage the access to. 


On the next modal window, search the user you want to give access to this model. 


Click the  corresponding to the user to grant access to the model. 


To revoke access to the model, click the delete icon corresponding to the user who already have access to this model. 


If you want to manage model access of a specific user, switch to “User View” (as shown in Figure 4.11b). 


Click the icon corresponding to the user you want to manage the access for. 


In the next modal window, search the model you want to grant this user access to and the click on the plus icon. 


If you want to revoke this user’s access to a particular model that this user already have access to, click on the delete icon corresponding to the model. Figure 4.11c shows a screenshot example. 


Figure 4.11c: User View showing all models a specific user access to and models that can be added to grant this user access to. 


About Impulse 

Impulse DW is a blazing fast and highly scalable SQL based data warehouse platform. It offers the following features:
Blazing fast and scalable database for ad hoc analytics and online analytical processing (OLAP)A fully integrated ETL platform to ingest data from a wide variety of data sources and formats, transform, and build an automated data pipeline to load and index data for interactive and ad hoc query.Fully integrated web-based visualization and BI engine to create interactive dashboards.JDBC and Restful APIs are available to connect with third party BI tools.The Restful API specification is available here: Restful API Specification


1.1 Signup 

To register a user:
In your browser window, type the URL and port number of Impulse server. The URL will vary for every installed instance of Impulse. If this is the first time after the system is installed, you may be able to open the login page by pointing your browser to http://<youripaddress>:433 (replace <youripaddress> with the actual IP address of the impulse server.On the login page, click “Sign up” link, fill out the registration form an click submit.
Import notes:
The first time user who registers using the above method becomes the default administrator of the system.An admin has all the privileges systemwide.An admin can not delete itself or change its role. If, during the installation process, impulse is configured to integrate with Momentum and Momentum is deployed within the Impulse cluster, the same user becomes the admin of Momentum as well.
The following depicts the registration form.
Figure 1.1a Signup screen


1.2 Password Change 

To change password:
From the main navigation menu, expand “Users and Roles” and click on “Profile and Password”The top of the page contains the form to change password.Provide your old and new passwords you want to set to.Click Change Password.
Note: You will not need to fill out the lower section of the form (profile section) to change the password.
Figure 1.2a Password change screenshot


1.3 Profile Management 

To change your profile information:
Left navigation menu –> expand Users and Roles –> Click Profile & Password –> Edit the profile section of the form. You will not need to update the password –> Update Profile button
Figure 1.3a Update profile



1.4 Forgot Password 

To reset your password:
Click Forgot Password link from the login pageProvide with the email address associated with the impulse accountAn email link to reset the password will be sent your email address.Click on the link sent via email.Provide the new passwords and submit.     
Note: You must set SMTP settings before the system can send emails. See System Configuration to learn how to set SMTP.






2.1 Create a Warehouse 

Data warehouse (or DW for short) is a collection of logically related tables or data sources. You create a DW to organize tables within it. 
To create a new data warehouse:
On the main navigation menu, click “Data Warehouses”. Click on the + icon located at the top left corner of the page (see Figure 2.1.a below)Fill out the form:Warehouse Name: This is a user defined name to recognize this warehouseWarehouse Alias: This is short name or code given by user to this warehouse. Ensure there is no space or special characters in the warehouse alias. All tables are referenced using the warehouse alias. For example, if the alias is “patientdw” and there are two tables — patient and schedule — within this warehouse, the tables are uniquely identified using the alias as patientdw.patient, and patientdw.schedule.Description: User defined description or purpose of this warehouse.Submit to create the DW. 
Figure 2.1.a: Screen indicating the + icon to create a new data warehouse
Figure 2.1b: Screen showing the warehouse fields with sample data entry


2.2 Edit Warehouse 

To edit a warehouse:
From the left navigation menu, click “Data Warehouses”Click the “Edit” link corresponding to the warehouse you wish to edit.Update the fields that you wish you edit and click Submit to save the updated information.


2.3 Datasources In Warehouse 

We organize tables or data sources within a data warehouse. To see a list of data sources within a DW:
On the main navigation menu, click Data WarehousesClick the link “Datasources” corresponding to the DW you are interested inThe next page will show a list of all data sources within the DW.
With each datasources, it shows the following important information about the datasource:
Partition Count: This shows the number of partitions this data source is split into.Partition size: Total size in bytes of all partitions combined. Dividing this number by the number of partitions will give the average partition size in bytes.Replication size: For multi-node replicated cluster, this will show the total partition size of all replicated data/partitions. For a single node cluster, the replication size will be the same as the partition size.Available %: This is shows the percentage of partitions available for querying. Ideally this should be 100%.
It is important to note that after your upload and index the data, the data will not be available for querying until this available percentage is not 100%.


2.4 Ingesting Data Into Tables or Datasources 

There are three different ways to create table or datasource in a data warehouse. These methods are described in the following sections:
File upload using drag-and-drop via Impulse UIIngest from external file systems such as S3, Hadoop and MomentumIngest using Momentum Data Pipeline
Please note that, in this document, we use “table” and “datasource” interchangeably.


2.4.1 Ingesting From Momentum Data Pipeline 

To ingest data into Impulse DW:
Create an Impulse emitter. See instructions here.Add the Impulse emitter to the end of a data pipeline. See instructions here.Run the data pipelineMonitor the pipeline status from the Pipeline Home page. As a pipeline component runs, the status of that component is shown as “Running” on the pipeline home.After all components of pipeline complete the execution, you will notice the status “complete” and the result “success” (See Figure 1 below)Open the Impulse DW UI –> click “Tasks” from the main menu options to monitor the indexing tasks with their completion statues. (See Figure 2 below)After all the indexing tasks are completed, your new index will be visible under the “Data Warehouses” tab on Impulse.
Figure 2.4.1a: Momentum ETL, Pipeline Home page showing the execution status and result.
Figure 2.4.1b: Impulse DW Tasks status page



2.4.2 Uploading File Using Impulse UI 

Impulse provides a convenient way to create a table and upload data to it. Data uploaded to impulse is partitioned and indexed for efficient query. This section describes how to upload data into a table using Impulse’s file upload mechanism.
Step 1: Upload Data
From the main navigation menu, click “Load Data”Drag and drop as many files as you want to upload to a table. You may browse and upload files as well. See Figure 2.4.2a below.Fill out the form (See Figure 2.4.2b below):Warehouse: from the drop down, select the data warehouse in which you wish to create the table.Datasource: Give a meaningful name to your datasource. The datasource is analogous to a table in RDBMS paradigm. If the table name within the selected warehouse exists, the  data will  be uploaded in the existing table, else a new table will be created.Input Source: Since we are uploading, leave the default selection as “Browse & Upload”. For other types of input source, see the appropriate sections of this document.File Format: Select the appropriate file format of the data file you are uploading. The supported file formats are:ParquetCSV or comma separated valuesTSV or tab separated valuesPSV or pipe separated valuesJSON (line delimited) meaning each line in the file represents a single rowInput Header: This field is optional. If your input file is delimited (csv, psv, tsv) and does not contain the field header as the first line in each uploaded file, provide a comma separated list of header. Leave this field empty if your input file contains the header, otherwise, the ingestion engine will try to ingest the first line as data and not as header.Click Next button to configure the indexing and partitioning of data for efficient query execution.
Step 2: Column Mapping and Partition Parameters
After clicking the “Next” button in step 1, the next page will shows the parameters for the step 2 (see Figure 2.4.2c for example). These parameters control how the data indexing and partitions will be created. The description of each field within this step is as follows:
**For the best result, use a date or time based column as the primary partition column. If none of the column can be parsed as a date/time, do not use any partition.**
Datasource: the table name (as set in step 1 above)Secondary Partition Strategy: This defines the column or columns that will be used to create the secondary partition. Impulse supports two types of secondary partition strategies:Dynamic: This is the best partition strategy and does the most efficient partitioning based on the data. In most cases, you will leave this as the default secondary partition strategy.Single Column: If your data will have only one column in the group by or where clause, this single column based strategy will likely to work the best. However, this is highly discouraged to use a single column based partitioning.Primary Partition Granularity: If you have a date/time based primary column, this parameter specifies how your data will be split into partitions. For example, if you select a “day” for the granularity level, the entire data will be grouped by day and split into partitions.Missing Datetime Placeholder: If you select a date/time based column as the primary partition column and if any of the rows contains invalid/missing/null values for the primary partition column, it will fill the missing value with this placeholder datetime.Max Parallel Upload Tasks: This parameter defines how many threads the system will create to upload the data in parallel. For a single node deployment, this should be set at maximum of 60% of number of available CPU cores in your server. For example, if you have 32-core CPU, set the max parallel tasks as 20 or less. For a distributed cluster nodes, this value should be  60% of the sum of cores of all worker nodes.Upload Mode: Specify whether you want to append rows to and existing table or overwrite existing partition.
Field Mapping: System will try to guess the datatypes of each column. In case of incorrect interpretation, you should edit the datatypes of every column that were incorrectly interpreted. Only the “STRING” “LONG” and “DOUBLE” datatypes are supported. Dates are represented as a STRING datatype.
From the field mapping section, select the primary partition column, preferably a datetime column.
you must specify the datetime format of the primary partition column. ISO date format and joda-time datetime (  https://www.joda.org/joda-time/key_format.html ) format are supported.
If your secondary partition strategy is “Single Column” based, you must select the secondary partition column.
At the bottom of the page, the system displays a few lines of actual data to help you to see the datatype, format and sample values of the actual dataset.
To start indexing, click the “Load and Index” button.
This will open the “Tasks” page that shows a list of all active or completed indexing tasks. See Figure 2.4.2d as an example.
Figure 2.4.2a: Browse or drag-and-drop to upload files to a datasource
Figure 2.4.2b: Screen showing file upload options, data warehouse name, and datasource name

Figure 2.4.2c: Screen showing data ingestion, field mapping, and partitioning parameters
Figure 2.4.2d: Screen showing task status after indexing is triggered



2.4.3 Ingesting From External File/Storage System 

Impulse supports ingesting data from the following external file systems:
Amazon S3: Ingest a file of files stored in S3 bucket. This is the default storage system if Impulse is running on Amazon EC2 or you purchased the impulse license from the AWS Marketplace.HDFS: Ingest file or files stored in Hadoop Distributed File System (HDFS)Momentum: Ingest data from Momentum storage. Momentum provides a highly scalable ETL, including data ingestion from a wide variety of sources, transformation, cleaning, blending, and merging with multiple sources. It also allows ingesting data in automated fashion and creating indexes in Impulse.Google Cloud Storage: This is the default storage system if Impulse is running on Google Cloud.
To ingest data from the external system, follow these steps:
From the main navigation menu, click “Load Data” (See Figure 2.4.3a below as an example)Fill out the form:Warehouse: Select the warehouse from the drop down optionsDatasource: Enter the table or datasource name.Input Source: Select the external system to ingest data from.Input Path: Provide the fully qualified path to the data directory or a single file. For example:Momentum: fully qualified component name, e.g. accure.tr.sampledataS3: absolute path of the file or directory, e.g. s3://mybucket/mydirGCS: absolute path of the file or directory, e.g. gs://mybucket/mydirHDFS: absolute path of the file or directory, e.g. hdfs://ip-address:port/directory/pathFile Format: Select the input file formatInput Header: Enter a comma separated list of header columns if the input format is CSV, TSV or PSV and the input files do not contain the header in the first line.Click Next and follow the Step 2 as described in the previous section Uploading File Using Impulse UI
Figure 2.4.3a: Screen showing the form fields for ingesting data from external systems



2.5 Add Data to Existing Tables 

To add data to existing an table:
From main navigation menu, click Data Warehouses Click the “Datasources” link corresponding to the warehouse that contains the table you want to upload data to (Figure 2.5a)Click the upload icon corresponding to the table name (Figure 2.5b)In the next page, you will notice that both the warehouse and table names are pre-populated. Follow the rest of the steps to upload and index data as described in the section Uploading File Using Impulse UI
Figure 2.5a: Screen showing a list of data warehouses
Figure 2.5b: Screen showing a list of tables within a warehouse. The upload icon is underlined with red line.



2.5.1 Update Existing Index 

Impulse does not support row level updates. You can overwrite partitions within a specific data range and add new data to existing index. Here is an example of how this works:
Assume you have an existing index in Impulse with the following rows:
DateEmployeeSalary2019-01-01John60002019-01-02Sara61002019-01-03Smith62002019-01-04Bob6300
If we have the following dataset that we want to append to existing index and update the existing partitions within the period 2019-01-01 and 2019-01-03:
DateEmployeeSalary2019-01-01Jessica70002019-01-02Sara71002019-01-03Simpson72002019-01-04Robert73002019-01-05Dave7400
The resultant data after the combine and overwrite of partitions within the period 2019-01-01 and 2019-01-03 will be:
DateEmployeeSalary2019-01-01John70002019-01-01Jessica70002019-01-02Sara71002019-01-03Simpson72002019-01-04Bob63002019-01-04Robert73002019-01-05Dave7400
Notice that the partitions within the date range 2019-01-01 and 2019-01-03 are replaced by the the new dataset and outside this date range the data are appended to existing index.
Here are the steps to combine and overwrite the existing partitions.
Follow the steps of ingesting files to a datasource in a warehouseIn the step 2, select the Upload Mode as “Combine & Overwrite: Combine old data with new data and overwrite” option from the dropdown list (see Figure 2.5.1a below)In the partition overwrite period from and to fields, fill out the data range that you want your new data to update to.Follow the remaining steps of ingesting data into Impulse.
Figure 2.5.1a: Screen showing upload mode as “Combine & Overwrite: Combine old data with new data and overwrite



2.6 Delete Table Records (Rows) 

Impulse data structure is optimized for efficient query execution. The index structure within Impulse makes it hard to delete a specific record or a group of records. Impulse provides a mechanism to delete table records using “Delete” statement as described below.
It is important to note that the delete operation is very expensive — it reindexes the entire datasource/table and, therefore, it is likely to take time and resources.
NOTE: Do not ingest any data to the same table from where the records are being deleted, until the delete operation is completed.  Ingesting data while delete operation is still on will cause the data loss (of the new data being ingested).
To delete a row or set of rows matching a criteria, write the following query in the SQL statement field and click run (as shown in Figure 2.6a below).
DELETE FROM "DW_ALIAS.TABLENAME" WHERE RecordDate BETWEEN '2000-01-01' AND '2021-12-31'

The where clause supports the following operators:
AND
OR
BETWEEN
The following conditional operators are supported:
OperatorCondition==equal to!=not equal to>greater than>=greater than or equal to <less than<=less than or equal to


2.7 Delete Tables or Datasources 

To delete a table, including all its data:
From the main navigation menu, click “Data Warehouses”Click “Datasources” corresponding to the warehouse that contains the table you want to deleteClick on the trashcan icon corresponding to the table (See Figure 2.6a below)Provide your username to confirm that you really want to delete the table and all data in it.
**Note that this is an irreversible process and the deleted data can not be recovered.**
Figure 2.7: Screen showing a list of tables within a warehouse. Also showing the trashcan icon (highlighted in green) used to delete the table



2.8 Monitoring Indexing Tasks 

To monitor the status of indexing tasks:
Click Tasks from the main navigation menuThe next screen will show you a list of tasks with their statuses — RUNNING, SUCCESS, FAILURE (Figure 2.8a)
An indexing task creates several sub-tasks depending on the data size and the number of partition splits. 
You may notice concurrently running multiple sub-tasks, depending on the max parallel tasks setting during the ingestion configuration.
Figure 2.8: Screen showing the task statuses


2.9 View Datasource Stats 

The partition stats shows the number of partitions created for a selected datasource. Here are the steps to visualize the partition stats:
Main navigation menu –> click Data Warehouses. This will shows a list of warehousesClick on the Datasources link corresponding to a warehouse. This will show all datasources within a warehouse.Corresponding to a datasource (or table name), click on the “Stats” icon (as shown in Figure 2.7a below).The Stats screen shows the following three types of information (also see Figure 2.7b below):Partition Count: The bar chart shows the number of partitions created over time. The x-axis shows the ingestion date and the y-axis shows the number of partitions created.Partition Size: The line chart shows the size of partitions over time. The x-axis is the ingested time and y-axis is the partition size in Bytes.Partition Details: A tabular view of partition details containing the partition version, partition number, creation date and partition start and end intervals.
Figure 2.9a: Screenshot showing a list of datasources within a warehouse. The Stats icons are highlighted with red marker for demonstration only.
Figure 2.9b: Screenshot showing a datasource stats – partition count over time, partition size over time and a detailed list of partitions.



3.1 MVInsight Integration with Impulse 

A detailed tutorial will be added soon.


3.2 Tableau Integration with Impulse 

Impulse is JDBC compliant. You can query Impulse DW using Avatica JDBC driver.
To query Impulse from Tableau:
Download the Avatica JDBC driver jar from Avatica JDBC driver from Maven RepositoryCopy the driver jar to Tableau’s Drivers directory. The directory location depends on your operating system and location that you chose during the Tableau installation. For example, in Windows based system and Tableau Desktop, the default location may be C:\Program Files\Tableau\Drivers.Restart Tableau.On Tableau Desktop, under “To a Server” heading, click More and search for “Other Databases (JDBC)” as shown in Figure 1 below.Click “Other Databases (JDBC)”  and enter the Impulse URL in this format jdbc:avatica:remote:url=http://impulse-ip:port/impulse/v2/sql/avatica/. The URL and port combination is the same as the URL you use to access Impulse web console. If you SSL is used to securely access Impulse, make sure to change the http to https for the URL scheme. Select SQL 92 Dialect from the dropdown. See Figure 3.2a below.Provide the impulse authorized username and password and click Sign In button.From the list of databases, select “impulse” database from the dropdown menu. Then select “impulse” from the Schema menu. This should show a list of data warehouse tables.Drag the table/tables to the main body Tableau that says something “Drag tables here” to create your Tableau data source.
Consult Tableau documentation for details on how to work with data sources and create dashboards.
Figure 3.2a. Tableau Desktop showing Other Databases (JDBC)
Figure 3.2b. JDBC connection settings



4. Security, Roles and Privilege Management 

Momentum provides authentication, authorization and access control at very granular level. This section describes access control features of Impulse.
User Group
Users belong to one of the following groups:
AdminWriterReaderReaderWriter
The access level of the members of these groups are described in the following table:
FunctionalityAdminReaderWriterReaderWriterData warehouseCreateyesnoyesyesEdit yesnoyesyesView detailsyesnoyesyesAdd tablesyesnoyesyesLoad DatayesnoyesyesQueryyesyesyesyesManage UseryesnononoManage Rolesyesnonoowner DW onlyMonitor TasksyesnoyesyesSQLyesyesyesyesAPI: QueryyesyesyesyesSystem ConfigyesnononoSystem Servicesyesnonono

Permission Types
Entire warehouse: all tables within warehouse accessibleTable or selected group of tables: only the included tables are accessible
Default role: <USER>_OWNER and assigned by default to the user who creates a warehouse
Events, Permissions, and Roles
System automatically creates and delete roles when certain events are triggered. The following table outlines events and different roles that are created or deleted.
EventsActionsUser signs up<USER>_OWNER role is auto createdCreate a DW1. RW permission <DW>_RW is created2. <USER_OWNER> role is assigned Delete DW1. <DW>_RW permission is removedDelete Table1. <TABLE>_RW permission is removed


4.1 Enable SSL 

This section describes steps to secure the impulse access over SSL using https protocol. When you install Impulse the first time, the web console is accessible via http though on port 443. The port 443 is the default port for SSL but because there is no SSL certificate installed, the web console is served over http.
To enable SSL, you must have SSL certificate and key files for the domain you want to use to access the web console. The domain or subdomain must be pointed to the impulse IP address. See your domain provider’s documentation on how to point a domain/subdomain to an IP address. 
This tutorial assumes that you have a domain/subdomain and corresponding certificate and key files. Here are the steps:
Login using an admin account to Impulse web consoleFrom the main navigation, click on System Config (Figure 4.1a)In the next page, check the box for “Enable SSL” Browse and upload the certificate and key filesSave the config 
If everything goes well, the next page shows a confirmation message and instructions to restart the web server.
Click the “Restart” link to restart the server. (Figure 4.1b)
It may take about a minute to restart the server. The page will reload and auto navigate to the login page.
Figure 4.1a: Screen showing (only the top portion) the configuration form to enable SSL and upload the cert and key files
Figure 4.1b: Screen showing the server restart button.



4.2 Securing Backend SQL Engine 

You must change the default impulse user and password for the security reason.
You must be an admin to perform the following steps:
Click System Config from the main navigation menuAs shown in Figure 4.2 below, locate the following fields and provide secured username and password. For security reason, both the user and password are never displayed. Make sure you key-in the username and password correctly.IMPULSE_DB_SYS_USER: a default admin user (do not use admin or sysadmin)IMPULSE_DB_SYS_PASS: enter a complex password that can not be gassed or be a victim of random attack.Save the configIn the next page, restart the web server.After the web console is reloaded (after the restart), stop and start the backend services. Click System Services and stop the following services one by one:impulse-httpimpulse-masterimpulse-workerStart the above three services one by one
Your new username and password will apply to the backend SQL engine.
Figure 4.2: Screen showing a portion of the system config form, indicating system user and password fields.



4.3 Sharing and Access Control 

Follow the two-step process to share a warehouse or datasource with another user.
Step 1: Create an access role to resources you want to share
Step 2: Assign the role to a user


4.4 Add User 

To add a user and grant access to warehouses and tables, follow the steps:
Expand Users and Roles from the main navigation menuClick Manage UsersClick the plus icon located at the top of the page (see Figure below)Fill out the New User form, as described belowFirst Name: First name of the userLast Name: Last name of the userUsername: a unique username without any space or special characters. This should not be an email.Password: Give a default password that user must change on the first logonEmail: User email, must be unique email not used with any other userGroup: Select the user group. Learn more about the group and privilegeAt the bottom of new user form, select the checkboxes corresponding to roles to assign one or more roles to this userClick Save
Figure 4.4: Add user


4.5 Edit User 

This section describes how to edit user information, reset password, and assign/revoke access roles. You must be an admin to perform the following steps:
Main navigation menu –> Expand Users and Roles –> Manage UsersClick Edit link located at the far right column of the user you want to editYou can update user information and save.
Reset User Password
There are two ways to reset user password:
Send Password Reset Link: This is a secure way to send a link to user’s email. The user should follow the link to change their password. This feature will work only if the email service (SMTP config) is configured.Generate Temp Password: This will create a random password that you can share with the user. User must change this password on the first login.
Assign or Revoke Access Roles
To add additional roles to this user, check the boxes corresponding to the role/roles you want to assign to this user.To remove roles, uncheck the box you want to remove the roles from this user’s access.Click Update button to save the changes.
Figure 4.5a: Screen showing user information and editable fields.


4.6 Create Role 

To create a role:
Expand Users & Roles from the main navigation menuClick Manage Role (Figure 3.3a below)On the next page click on the plus icon to create a new roleFill out the form to provide (Figure 3.3.b):Role Name: a short name to identify this roleRole Description: A descriptive name to indicate its purposeSelect the Read or/and Write checkboxes corresponding to a warehouse or one or more tables to define the access privilege under this new role.Save to create the role.
Note: This role needs to be assigned to one or more users to grant access to resources to those users. Here is the instruction on how to assign roles to users.
Figure 4.6a: Screen showing the Users and Roles menu and icon (plus symbol at the top) to create a new role
Figure 4.6b: Screen showing the form to create role to define access privilege to warehouses/datasources



4.7 Delete Role 

You must have the appropriate privilege to delete a role. Perform the following:
Main navigation menu –> expand Users & Roles –> Manage RolesOn the next page, click on the Delete link corresponding to the role you want to delete (as shown in Figure 3.7a)A confirmation dialog will open to receive your confirmation on this delete operation. 
Once you delete a role, all users having this role assigned to them will no longer have access to the resources assigned under this role.
Figure 4.7a: Screen showing a list of roles with delete buttons with each role.



4.8 Assign Role to User 

In order to grant access of resources (warehouse and tables) to a user, assign a role to the user. The following steps describe the process of assigning roles to a user. Before proceeding with the following ensure that you have already created roles . To learn how to create a role follow the tutorial Create Role.
Assign Roles to a New User
Expand Users and Roles from the main navigation menuClick Manage UsersClick the plus icon located at the top of the page (see Figure 3.5a below)Fill out the New User form. See Create User to learn more about the form fields and their meanings.At the bottom of new user form, select the checkboxes corresponding to roles to assign one or more roles to this user (Figure 3.5b below)Click Save
Assign Roles to an Existing User
As shown in Figure 3.5a, click on the Edit link corresponding to the user you want to assign one or more roles toOn the next page, click checkboxes corresponding to one or more roles.If a role is already assigned to this user and you want to keep that role assigned, leave the checkbox checked. Else, uncheck the box.Click Save
Figure 4.8a: Screen showing a Manage Users menu, add new user icon and list of users with Edit link corresponding to each user
Figure 4.8b: Screen showing the form to create new user and assign roles to data warehouses/tables



4.9 Edit User Privilege 

See the section Edit User to edit the user privileges.


5.1 System Configuration 

You must be an admin to be able to update system configuration. Changes made to the system config requires the web sever and other services to restart. The following table shows what services need to be started based on what config parameters are changed.
ParameterExplanationWeb serverImpulse-httpImpulse-masterImpulse-workerEnable SSL AccessEnable httpsyesnononoHTTP(S) Porthttp port numberyesnononoDB_HOSTLeave default unless the web console db is hosted outside the machine the application server is hosted inyesnononoEMAIL_HOSTSMTP host nameyesnononoEMAIL_PORTSMTP Port.yesnononoEMAIL_HOST_USERSMTP authorized usernameyesnononoEMAIL_HOST_PASSWORDSMTP authorized user passwordyesnononoSENDER_EMAILAll emails will be sent by this sender_emailyesnononoMOMENTUM_INTEGRATIONTrue/False to indicate whether Momentum is installed and used with ImpulseyesnononoMOMENTUM_API_URLIf Momentum is integrated, provide the Momentum API URL with port numberyesnononoIMPULSE_DB_URLURL with port number of the impulse-http server.yesnononoIMPULSE_DB_SYS_USERThis is the default admin user of the impulse databaseyesyesyesyesIMPULSE_DB_SYS_PASSPassword of the default adminyesyesyesyesIMPULSE_INDEX_DML_URLURL with port number of Impulse-master serveryesnononoIMPULSE_AUTH_URLURL with port number of Impulse-master serveryesnononoSTORAGE_TYPEsupported storage type: hdfs, s3, gc and localyesyesyesyesBUCKETBucket name in case of s3 and gcyesyesyesyesSTORAGE_PATHDirectory path inside the storage bucket of hdfs locationyesyesyesyesaws_access_key_idS3 access keyyesyesyesyesaws_secret_access_keyS3 access passwordyesyesyesyes


5.2 Managing System Services 

You must be an admin to perform the following operations to monitor system services and manage their running statuses.
Main navigation menu –> System ServicesYou will see all system services and their running statuses.Depending upon the status of a service, Start or Stop button will show enabled or disables. For example, if a service is up and running, the Start button will be disabled and the Stop button will be enabled.The Zookeeper (or zk) service is almost never needed to be stopped or restarted. However, if you see any zookeeper connectivity issue in the system logs, it may be a good idea to stop and start the zk service.Impulse-http, master and worker services need to be stopped and restarted only in case of any config change as described in the System Configuration sectionDo not stop the “impulseui” service unless there is any emergency and service is totally unresponsive or if there is any error. Doing so will stop the web server and the web console will not be accessible. If that situation arises, you will need to reboot the machine (OS).
If due to any reason, the web console is inaccessible, reboot your server (OS level reboot). All services should come back up.
Figure 5.2a: Screen showing System Services menu and service running statuses



6. Impulse DW Restful API 

Impulse DW Restful API provides a set of functionality to ingest data into impulse and execute SQL statement to get resultset in various formats.
To use the API, you must obtain the API token for a secure connection.
Next:
API TokenAPI Reference


6.1 API Token 

The API token is required for all API calls. To get the token:
Login to your account using the impulse web console.Left hand side menu option –> click on “APIs” linkKey-in your account password and click on “Reveal Key” buttonCopy the API key that is displayed when the reveal button is clicked.
Figure 6.1:API Key



6.2 API Reference 

Use Impulse DW Restful APIs to access data available in the data warehouse. The following section provides the service, request and response specification.
OperationRequest ParametersResponseResponse/v1/sql/resultRequest headerAuthorization: Token <your api token>JSON formatted request body{“sql”:”Your SQL statement”,”start_index”: [optional],”end_index”:[optional],”row_limit”:[optional, default=100],”result_format”:[optional, supported formats: default=json, csv, html]}Query result in the format specified.Exceptions:{“result”:”No result fetched”,”exception”:<Exception message describing the reason>”}
Examples:
Example1: curl -X POST https://impulsedw.accure.ai/v1/sql/result -H ‘Authorization: Token 1234567890987654321’ -d ‘{“sql”:”select * from \”tablename\””}’
The above example will return 100  rows in JSON format
Example 2: curl -X POST https://impulsedw.accure.ai/v1/sql/result -H ‘Authorization: Token 1234567890987654321’ -d ‘{“sql”:”select * from \”tablename\”, “row_limit”:1000, “result_format”:”csv”}’
This will return 1000 rows in csv format
Example 3: curl -X POST https://impulsedw.accure.ai/v1/sql/result -H ‘Authorization: Token 1234567890987654321’ -d ‘{“sql”:”select * from \”tablename\”, “start_index”:100, “row_limit”:1000, “result_format”:”csv”}’
This will return 1000 rows with off set = 100, in csv format
Note: Your URL may be different from the URL used in this document. You will need to use the URL for your deployment.


7. Release Notes 

Impulse is a column based database for fast query execution and analytics.
This section describes the new features, enhancements and bug fixes in the recent releases of Impulse.
Release date: December 10, 2021, version 2.2.19
Overwrite existing data with new datasetUpdate partitions (overwrite with new dataset) within a date range or periodDelete data warehouseVisualize partitions statsSecurity enhancementBug fixes
Release date: November 21, 2021, version 2.1.19
Inset BI integrationSecurity enhancementBug fixes
Release date: Oct 20, 2021, version 2.0.19
Amazon S3 integration as an ingestion sourceAmazon S3 integration as a primary source of backup, replication, and auto disaster recovery for impulse running on EC2Amazon AMI release for AWS marketplace
Release date: August 20, 2021, version 2.0.1
Security enhancement and bug fixesSecurity upgrade by allowing users to refresh API keysSecurity enhancement by hiding the API key and displaying only on demand and user authentication.Support for checking whether system services are running. For admins only.Support to “stop” and “start” of services from the UI. For admins only.Support for displaying system logs (tail top 100)
Release date: July 15, 2021, version 2.0.0
Security enhancementEnhancement in role, permission and access controlSupport for more granular level (entire warehouse vs individual tables) sharing and provision for read or write or readwrite access levelIntegration with Momentum ETL to ingest data from wide variety of data sourcesIntegration with Momentum Pipeline to index data into Impulse by either on-demand or scheduled based automationRole based access control for ingestion from MomentumIntegration with MVInsight visualization for graphing and dashboard creationJDBC based integration with BI tools, such as Tableau, Qlik and moreRestful API for ad-hoc query. Parameterized Rest request to retrieve data in JSON or CSV formatSupport for export of data in CSV format right from the UI
Release date: March 15, 2021, version 1.6.0
Support for ad hoc query from the browser based interfaceSecurity enhancement to encrypt all system passwords and sensitive user informationSupport for user defined http and https portsSupport for uploading client specific SSL certificates and management from UI by admin usersSupport for UI based configuration management by admin users
Release date: January 15, 2021, version 1.5.0
Web based user interface for user registration, password and account managementUI for roles, permissions, and privilege managementUI to create data warehouse and organize data sources within warehousesUI for drag-n-drop to add data sourcesUI for ETL to upload file, define fields, and set partition columnsUI to manage data sourcesSupport for ANSI SQL to query data sources
Release date: March 11, 2020, version 1.0
Creation of core engine for data ingestion, and partitioningCreation of clustering mechanism for scaling of data ingestionCreation of clustering mechanism for parallel and distributed query processing and result assemblySupport for Hadoop as a distributed file system for cluster based deployment.



7.1 Open Source Software Components and Libraries 

Impulse includes a number of subcomponents, open source software, libraries and databases. The following section provides a list of dependencies and open source software included in Impulse:
Apache Hive: https://hive.apache.org/Apache Lucene: https://lucene.apache.org/Apache Calcite: https://calcite.apache.org/Apache Calcite Avatica: https://calcite.apache.org/avatica/Apache Curator: https://curator.apache.org/Apache Derby: http://db.apache.org/derby/Apache HttpClient: https://hc.apache.org/Apache HttpCore:  https://hc.apache.org/Apache Log4j: https://logging.apache.org/log4j/2.x/Apache Maven: https://maven.apache.org/Apache Hadoop: https://hadoop.apache.org/Apache Zookeeper: https://zookeeper.apache.org/Apache DataSketches: https://datasketches.apache.org/Apache Avro: https://avro.apache.org/Apache Parquet: https://parquet.apache.org/Apache Directory: https://directory.apache.org/Apache Kafka: https://kafka.apache.org/Apache Velocity: https://velocity.apache.org/Apache commons: https://commons.apache.org/Modified version of Apache Druid: https://druid.apache.org/Apache ORC: https://orc.apache.org/Apache Ranger: https://ranger.apache.org/A modified version of the java-alphanum library, copyright Andrew Duffy: https://github.com/amjjd/java-alphanumA modified version of the Metamarkets java-util library, copyright Metamarkets Group Inc: https://github.com/metamx/java-utilA modified version of the CONCISE (COmpressed ‘N’ Composable Integer SEt) library, copyright Alessandro Colantonio: https://sourceforge.net/projects/concise/Guava: https://github.com/google/guavaNetflix Spectator, copyright Netflix, Inc. https://github.com/Netflix/spectatorApache Knox: https://knox.apache.org/pac4j: https://www.pac4j.org/AWS SDK for Java: https://aws.amazon.com/sdk-for-java/Esri Geometry API for Java: https://github.com/Esri/geometry-api-javaClassMate: https://mvnrepository.com/artifact/com.fasterxml/classmate/1.0.0Jackson: https://github.com/FasterXML/jacksonCaffeine: https://github.com/ben-manes/caffeineLMAX Disruptor: https://mvnrepository.com/artifact/com.lmax/disruptor/3.3.6LZF Compressor: https://github.com/ning/compressOpenCSV: http://opencsv.sourceforge.net/OkHttp: https://square.github.io/okhttp/Netty: https://netty.io/DropWizard Metrics Core: https://mvnrepository.com/artifact/io.dropwizard.metrics/metrics-core/4.0.0-alpha4Fastutil: https://fastutil.di.unimi.it/Joda-Time: https://www.joda.org/joda-time/Java Native Access (JNA): https://github.com/java-native-access/jnaPlexus Common Utilities: https://codehaus-plexus.github.io/plexus-utils/Hibernate: https://hibernate.org/SIGAR: https://mvnrepository.com/artifact/org.hyperic/sigar/1.6.5.132JBoss Logging: https://mvnrepository.com/artifact/org.jboss.logging/jboss-loggingJDBI: https://github.com/jdbi/jdbiMapDB: https://github.com/jankotek/mapdb/Objenesis: http://objenesis.org/RoaringBitmap: https://github.com/RoaringBitmap/RoaringBitmapGoogle APIs Client Library: https://developers.google.com/api-client-libraryKubernetes: https://kubernetes.io/Gson: https://github.com/google/gsonprotobuf: https://developers.google.com/protocol-buffers/docs/protoGoogle Compute Engine API: https://cloud.google.com/compute/docs/reference/rest/v1Microsoft Azure Storage Client SDK: https://docs.microsoft.com/en-us/java/api/overview/azure/storage?view=azure-java-stableSnappy Java: https://xerial.org/snappy-java/PostgresSQL: https://www.postgresql.org/Zstandard: http://facebook.github.io/zstd/ANTLR: https://www.antlr.org/Janino and Commons Compiler: https://janino-compiler.github.io/janino/ASM: https://asm.ow2.io/LevelDB JNI: https://github.com/fusesource/leveldbjniICU4J: https://mvnrepository.com/artifact/com.ibm.icu/icu4jSLF4J: http://www.slf4j.org/MurmurHash3: https://github.com/aappleby/smhasher
Python Libraries Installed from PyPI (https://pypi.org/):
pandasasgirefautopep8djangopycodestylepytzsqlparseUnipathdj-database-urlpython-decouplegunicornwhitenoisedjongopydruidSQLAlchemy==1.4.23setuptools==57.0.0gunicorndjangorestframeworklog4mongodjango-ipwarepyarrowfsspecs3fspycryptoboto3pydoop


Alerts and Reports 


Connecting to a new database  

First things first, we need to add the connection credentials to your database to be able to query and visualize data from it. If you’re using Inset BI independently, you can skip this step because a Postgres database, named PostgreSQL, is included and pre-configured in Inset for you. 
Under the Data menu, select the Databases option: 

Next, click the green + Database button in the top right corner: 

You can configure a number of advanced options in this window, but for this walkthrough you only need to specify two things (the database name and SQLAlchemy/Impulse DW URI): 


The Impulse DW URL format should be in the format: impulse://username:password@hostname:port/impulse/v2/sql 
Click the Test Connection button to confirm things work end to end. If the connection looks good, save the configuration by clicking the Finish button in the bottom right corner of the modal window. 
Congratulations, you’ve just added Impulse DW data source in Inset BI. 


Registering a new table 

Registering a new table 
Now that you’ve configured a data source, you can select specific tables (called Datasets in Inset) that you want exposed in Inset for querying. 
Navigate to Data ‣ Datasets and select the + Dataset button in the top right corner. 

A modal window should pop up in front of you. Select your Database, Schema, and Table using the drop downs that appear. In the following example, we register the Vehicle Sales table from the examples(postgresql) database. 

To finish, click the Add button in the bottom right corner. You should now see your dataset in the list of datasets. 
Customizing column properties 
Now that you’ve registered your dataset, you can configure column properties for how the column should be treated in the Explore workflow: 

Is the column temporal? (Should it be used for slicing & dicing in time series charts?) 


Should the column be filterable? 
Is the column dimensional? 
If it’s a datetime column, how should Inset parse the datetime format? (Using the ISO-8601 string pattern) 


Inset Semantic Layer 
Inset has a thin semantic layer that adds many quality-of-life improvements for data scientists and analysts. The Inset semantic layer can store 2 types of computed data: 

Virtual metrics: you can write SQL queries that aggregate values from multiple column (e.g. SUM(recovered) / SUM(confirmed)) and make them available as columns for (e.g. recovery_rate) visualization in Explore. Aggregate functions are allowed and encouraged for metrics. 


You can also certify metrics if you’d like for your team in this view. 

Virtual calculated columns: you can write SQL queries that customize the appearance and behavior of a specific column (e.g. CAST(recovery_rate) as float). Aggregate functions aren’t allowed in calculated columns. 




Creating charts in Explore view 

 
Inset has 2 main interfaces for exploring data: 

Explore: no-code viz builder. Select your dataset, select the chart, customize the appearance, and publish. 
SQL Lab: SQL IDE for cleaning, joining, and preparing data for Explore workflow 

We’ll focus on the Explore view for creating charts right now. To start the Explore workflow from the Datasets tab, start by clicking the name of the dataset that will be powering your chart. 

You’re now presented with a powerful workflow for exploring data and iterating on charts. 

The Dataset view on the left-hand side has a list of columns and metrics, scoped to the current dataset you selected. 
The Data preview below the chart area also gives you helpful data context. 


Using the Data tab and Customize tabs, you can change the visualization type, select the temporal column, select the metric to group by, and customize the aesthetics of the chart. 

As you customize your chart using drop-down menus, make sure to click the Run button to get visual feedback.  
In the following screenshot, we craft a grouped Time-series Bar Chart to visualize our quarterly sales data by product line just be clicking options in drop-down menus. 

Creating a slice and dashboard 
To save your chart, first click the Save button. You can either: 

Save your chart and add it to an existing dashboard 
Save your chart and add it to a new dashboard 

In the following screenshot, we save the chart to a new “Sales Dashboard”: 

To publish, click Save and go to Dashboard. 
Behind the scenes, Inset will create a slice and store all the information needed to create your chart in its thin data layer (the query, chart type, options selected, name, etc). 

To resize the chart, start by clicking the pencil button in the top right corner. 

Then, click and drag the bottom right corner of the chart until the chart layout snaps into a position you like onto the underlying grid. 

Click Save to persist the changes. 
Congrats! You’ve successfully linked, analyzed, and visualized data in Inset BI. There are a wealth of other table configuration and visualization options, so please start exploring and creating slices and dashboards of your own 


Manage access to Dashboards  

Access to dashboards is managed via owners (users that have edit permissions to the dashboard) 
Non-owner users’ access can be managed in two different ways: 

Dataset permissions – if you add to the relevant role permissions to datasets it automatically grants implicit access to all dashboards that use those permitted datasets 


Dashboard roles – if you enable DASHBOARD_RBAC feature flag then you be able to manage which roles can access the dashboard 


Having dashboard access implicitly grants read access to the associated datasets, therefore all charts will load their data even if the feature flag is turned on and no roles are assigned to roles the access will fall back to Dataset permissions 


Customizing dashboard 
The following URL parameters can be used to modify how the dashboard is rendered: 

standalone: 


0 (default): dashboard is displayed normally 
1: Top Navigation is hidden 
2: Top Navigation + title is hidden 
3: Top Navigation + title + top level tabs are hidden 


show_filters: 


0: render dashboard without Filter Bar 
1 (default): render dashboard with Filter Bar if native filters are enabled 


expand_filters: 


(default): render dashboard with Filter Bar expanded if there are native filters 


0: render dashboard with Filter Bar collapsed 
1: render dashboard with Filter Bar expanded 

